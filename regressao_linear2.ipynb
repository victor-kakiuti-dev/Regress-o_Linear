{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPM1LsQTspM7vPkZD7YeIZn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victor-kakiuti-dev/Regressao_Linear/blob/main/regressao_linear2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressão Linear 2"
      ],
      "metadata": {
        "id": "i3YceVNcb0To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dessa vez foi uma implementação um pouco diferente"
      ],
      "metadata": {
        "id": "lqkkujwTb0G9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OJwtZWVcW9Zx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Dados de exemplo (y = 2x + 1 com um pouco de ruído)\n",
        "X = np.random.rand(100, 1)   # 100 pontos entre 0 e 1\n",
        "y = 2 * X + 1 + 0.1 * np.random.randn(100, 1)\n",
        "\n",
        "# 2. Inicializar parâmetros (peso e bias)\n",
        "w = np.random.randn(1)\n",
        "b = np.random.randn(1)\n",
        "\n",
        "# 3. Hiperparâmetros\n",
        "lr = 0.01\n",
        "epochs = 2000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  forward = X*w+b\n",
        "\n",
        "  loss = np.mean((forward - y)**2)\n",
        "\n",
        "  gradient_w = np.mean(2*(forward - y)*X)\n",
        "\n",
        "  gradient_b = np.mean(2*(forward - y))\n",
        "\n",
        "  w = w - lr*(gradient_w)\n",
        "  b = b - lr*(gradient_b)\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch: {epoch}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a_vDQt33XIUB",
        "outputId": "d23c898a-dd34-4c43-ad2d-f971de16a547"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 0.4514\n",
            "Epoch: 100, Loss: 0.3340\n",
            "Epoch: 200, Loss: 0.2613\n",
            "Epoch: 300, Loss: 0.2050\n",
            "Epoch: 400, Loss: 0.1613\n",
            "Epoch: 500, Loss: 0.1273\n",
            "Epoch: 600, Loss: 0.1010\n",
            "Epoch: 700, Loss: 0.0806\n",
            "Epoch: 800, Loss: 0.0647\n",
            "Epoch: 900, Loss: 0.0524\n",
            "Epoch: 1000, Loss: 0.0428\n",
            "Epoch: 1100, Loss: 0.0354\n",
            "Epoch: 1200, Loss: 0.0296\n",
            "Epoch: 1300, Loss: 0.0252\n",
            "Epoch: 1400, Loss: 0.0217\n",
            "Epoch: 1500, Loss: 0.0190\n",
            "Epoch: 1600, Loss: 0.0169\n",
            "Epoch: 1700, Loss: 0.0153\n",
            "Epoch: 1800, Loss: 0.0140\n",
            "Epoch: 1900, Loss: 0.0130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"W: {w[0]}, B: {b[0]}, Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CF__pdJ1YDYm",
        "outputId": "8f685e55-3519-4866-c781-d3679c82baf4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: 1.7446542145382384, B: 1.1313695046711736, Loss: 0.012282871062204916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obs: Aqui usamos Batch Gradient Descent: usa todos os dados"
      ],
      "metadata": {
        "id": "oddpZ_uIbRoc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hEHzBEgVapXV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}